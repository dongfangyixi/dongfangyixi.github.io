<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Reinforcement learning | Chandler’s Blog</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Reinforcement learning" />
<meta name="author" content="Xu LI" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Exploration in reinforcement learning" />
<meta property="og:description" content="Exploration in reinforcement learning" />
<link rel="canonical" href="http://localhost:4000/original/2018/09/09/Reinforcement-learning.html" />
<meta property="og:url" content="http://localhost:4000/original/2018/09/09/Reinforcement-learning.html" />
<meta property="og:site_name" content="Chandler’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-09-09T23:03:25+08:00" />
<script type="application/ld+json">
{"description":"Exploration in reinforcement learning","author":{"@type":"Person","name":"Xu LI"},"@type":"BlogPosting","url":"http://localhost:4000/original/2018/09/09/Reinforcement-learning.html","headline":"Reinforcement learning","dateModified":"2018-09-09T23:03:25+08:00","datePublished":"2018-09-09T23:03:25+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/original/2018/09/09/Reinforcement-learning.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Chandler's Blog" /></head>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    
  <body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Chandler&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Reinforcement learning</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-09-09T23:03:25+08:00" itemprop="datePublished">Sep 9, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="exploration-in-reinforcement-learning">Exploration in reinforcement learning</h1>

<h2 id="exploration">Exploration</h2>

<h3 id="-epsilon--greedy">$ \epsilon $-greedy</h3>

<p>$\epsilon$-greedy perhaps the implimentational easiest way to conduct exploration in reinforcement learning (RL). However it achieves amazing performance in most RL tasks.</p>

<p>greedy means take what the policy or Q function tells you to do</p>

<p>$\epsilon$ means that a random action is taken with a probability of $\epsilon$ .</p>

<script type="math/tex; mode=display">% <![CDATA[
a=\left\{
\begin{array}{cl}
\pi(s), & & in & &{p(1 - \epsilon)} \\
random(A), & &in&&{p(\epsilon)}
\end{array}
\right. %]]></script>

<h3 id="boltzmann-gibbs-softmax-action-selection">Boltzmann (Gibbs, Softmax) action-selection</h3>

<p>In Boltzmann exploration, we would ideally like to exploit all the information present in the estimated Q-values produced by our network. <em>Boltzmann exploration</em> does just this. Instead of always taking the optimal action, or taking a random action, this approach involves choosing an action with weighted probabilities. To accomplish this we use a softmax over the networks estimates of value for each action. In this case the action which the agent estimates to be optimal is most likely (but is not guaranteed) to be chosen. The biggest advantage over e-greedy is that information about likely value of the other actions can also be taken into consideration.</p>

<p><strong>Adjusting during training</strong>: In practice we utilize an additional temperature parameter (τ) which is annealed over time. This parameter controls the spread of the softmax distribution, such that all actions are considered equally at the start of training, and actions are sparsely distributed by the end of training.</p>

<h3 id="discovery-of-subgoals">Discovery of Subgoals</h3>

<p>Amy Mcgovern and Andrew G. Barto. Automatic discovery of subgoals in reinforcement learning using diverse density. In Proc. of ICML, 2001.</p>

<p>Sandeep Goel and Manfred Huber. Subgoal discovery for hierarchical reinforcement learning using learned policies. In Ingrid Russell and Susan M. Haller, editors, FLAIRS Conference, pages 346–350. AAAI Press, 2003.</p>

<h3 id="count-based">Count based</h3>

<p>Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. #exploration: A study of count-based exploration for deep reinforcement learning. CoRR, abs/1611.04717, 2016.</p>

<p>Jarryd Martin, Suraj Narayanan Sasikumar, Tom Everitt, and Marcus Hutter. Count-based exploration in feature space for reinforcement learning. CoRR, abs/1706.08090, 2017.</p>

<h3 id="use-approximations-of-a-state-transition-graph-to-exploit-structural-patterns">Use approximations of a state-transition graph to exploit structural patterns</h3>

<p>Sridhar Mahadevan. Proto-value functions: Developmental reinforcement learning. In Proceedings of the 22Nd International Conference on Machine Learning, ICML ’05, pages 553–560, New York, NY, USA, 2005. ACM.</p>

<p>Marlos C. Machado, Marc G. Bellemare, and Michael H. Bowling. A laplacian framework for option discovery in reinforcement learning. CoRR, abs/1703.00956, 2017.</p>

<h3 id="reward-bonus">Reward “bonus”</h3>

<p>takes a Bayesian approach by maintaining a model of the dynamics of the environment, obtaining a posterior of the model after taking an action, and using the KL divergence between these two models as a bonus. The intuition behind this approach is that encouraging actions that make large updates to the model allows the agent to better explore areas where the current model is inaccurate.</p>

<p>Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Curiosity-driven exploration in deep reinforcement learning via bayesian neural networks. CoRR, abs/1605.09674, 2016.</p>

<p>Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. CoRR, abs/1705.05363, 2017.</p>

<p>Fernando Fernandez and Manuela Veloso. Probabilistic policy reuse in a reinforcement learning agent. In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS ’06, pages 720–727, New York, NY, USA, 2006. ACM.</p>

<h3 id="evolution-strategy">Evolution strategy</h3>

<script type="math/tex; mode=display">\begin{aligned}
a_i^j = \sum_{i=1}^{15}f_6 
\end{aligned}</script>

<ul>
  <li>
    <p>if we want to start our work with reinforcement learning we should know that if one want to go one should know where to go</p>
  </li>
  <li>
    <p>How to make the Eq in the middle of the page</p>
  </li>
</ul>

  </div><a class="u-url" href="/original/2018/09/09/Reinforcement-learning.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col one-half">
      <h2 class="footer-heading">Chandler&#39;s Blog</h2>
        <ul class="contact-list">
          <li class="p-name">Xu LI</li><li><a class="u-email" href="mailto:dongfangyixi@gmail.com">dongfangyixi@gmail.com</a></li></ul>
      </div>

      <div class="footer-col one-half">
        <p>This is a website of my tech blogs. Recording several awesome tech in computer science and Artificial Intelligence. 一个关于AI的网站
</p>
      </div>

      <div class="social-links"><ul class="social-media-list"><li><a href="https://github.com/jekyll" title="jekyll"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a href="https://twitter.com/jekyllrb" title="jekyllrb"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li><li><a href="/feed.xml" title="rss"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#rss"></use></svg></a></li></ul>
</div>
    </div>

  </div>

</footer>
</body>

</html>
